\documentclass[a4paper,10pt]{article}
\usepackage{solutions}

\begin{document}
%%% fill in your team name and each student's name:
\frontpage{Hadamard}{Leonardo Romor, Maurice Frank, Simone Astarita, XinYu Fu, Yije Zhang}{1}
\bigskip
\noindent
Unless otherwise stated, you should provide exact answers rather than rounded numbers (e.g., $\log 3$ instead of 1.585) for non-programming exercises.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{nproblem}{5}{Entropy of functions of a random variable}
Let $X$ be a random variable, and let $f$ be a function of $X$.
\end{nproblem}

\begin{subproblem}{2}
Show that $H(f(X) | X) = 0$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{3}
Show that $H(f(X)) \leq H(X)$. \\\textbf{Hint:} use the chain rule.
\end{subproblem}

\begin{solution}
...
\end{solution}



\begin{nproblem}{8}{Relative and cross entropy}
\end{nproblem}

\begin{subproblem}{5}
Prove that for any two distributions $P$ and $Q$ over $\mathcal{X}$, $D(P||Q) \geq 0$, and that equality holds if and only if $P = Q$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{1}
We have seen that the mutual information can be expressed in
        terms of the relative entropy, i.e.\ that $I(X;Y) =
        D(P_{XY}||P_X \cdot P_Y)$. Use (a) and this fact to show that $H(X|Y) \leq H(X)$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{2}
We have seen a relation between relative entropy and cross
        entropy in Intro/Team Quiz 02. Use this relation to express
        the mutual information $I(X;Y)$ in terms of Shannon entropies
        of $X$ and $Y$ (such as $H(X), H(Y), H(X|Y), H(Y|X), H(YX)$) and of the cross entropy $H_c$ of $P_{XY}$ and $P_X
        \cdot P_Y$.
\end{subproblem}

\begin{solution}
...
\end{solution}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{nproblem}{8}{Programming}
	\textbf{Note:} You do not have to hand in your code. As before, describe briefly what you did to reach your answer, what choices you made, and how you treated edge cases if those arose.
\end{nproblem}

\begin{subproblem}{1}
In the \href{https://canvas.uva.nl/courses/10933/assignments/72716}{first programming quiz} for this module, you computed the entropy of sampling a single letter from the story of Jack and the beanstalk.

If you instead decided to sample a single \emph{word} from the same story, do you expect the entropy to be higher or lower compared to sampling a single letter? Why?
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{3}
Verify your answer from (a) by writing a program that computes the entropy explicitly. Use \href{https://raw.githubusercontent.com/cschaffner/InformationTheory/master/2019/Text/Jack.txt}{the same text file} as input.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{1}
In the \href{https://canvas.uva.nl/courses/10933/assignments/72715}{second programming quiz} for this module, you computed the entropy of the second letter given the first when sampling a pair of letters from the story of Jack and the beanstalk. Let $P_{XY}$ denote the joint distribution where $X$ is the first and $Y$ is the second letter (again negleticing the beginning and end of the text).

Explain in words why $P_X = P_Y$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{2}
For $P_{XY}$ the bigram distribution from above, write a program to compute the cross entropy $H_C(P_{XY} , P_X \cdot P_Y)$ between the bigram distribution and the independent single-letter distribution.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{1}
Express in words what this quantity  $H_C(P_{XY} , P_X \cdot P_Y)$ means.
\end{subproblem}

\begin{solution}
...
\end{solution}

\end{document}
\documentclass[a4paper,10pt]{article}
\usepackage{solutions}

\begin{document}
%%% fill in your team name and each student's name:
\frontpage{Your team name}{Your names}{2}
\bigskip
\noindent
Unless otherwise stated, you should provide exact answers rather than rounded numbers (e.g., $\log 3$ instead of 1.585) for non-programming exercises.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{nproblem}{5}{Entropy of functions of a random variable}
Let $X$ be a random variable, and let $f$ be a function of $X$.
\end{nproblem}

\begin{subproblem}{2}
Show that $H(f(X) | X) = 0$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{3}
Show that $H(f(X)) \leq H(X)$. \\\textbf{Hint:} use the chain rule.
\end{subproblem}

\begin{solution}
...
\end{solution}



\begin{nproblem}{8}{Relative and cross entropy}
\end{nproblem}

\begin{subproblem}{5}
Prove that for any two distributions $P$ and $Q$ over $\mathcal{X}$, $D(P||Q) \geq 0$, and that equality holds if and only if $P = Q$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{1}
We have seen that the mutual information can be expressed in
        terms of the relative entropy, i.e.\ that $I(X;Y) =
        D(P_{XY}||P_X \cdot P_Y)$. Use (a) and this fact to show that $H(X|Y) \leq H(X)$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{2}
We have seen a relation between relative entropy and cross
        entropy in Intro/Team Quiz 02. Use this relation to express
        the mutual information $I(X;Y)$ in terms of Shannon entropies
        of $X$ and $Y$ (such as $H(X), H(Y), H(X|Y), H(Y|X), H(YX)$) and of the cross entropy $H_c$ of $P_{XY}$ and $P_X
        \cdot P_Y$.
\end{subproblem}

\begin{solution}
...
\end{solution}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{nproblem}{8}{Programming}
	\textbf{Note:} You do not have to hand in your code. As before, describe briefly what you did to reach your answer, what choices you made, and how you treated edge cases if those arose.
\end{nproblem}

\begin{subproblem}{1}
In the \href{https://canvas.uva.nl/courses/10933/assignments/72716}{first programming quiz} for this module, you computed the entropy of sampling a single letter from the story of Jack and the beanstalk.

If you instead decided to sample a single \emph{word} from the same story, do you expect the entropy to be higher or lower compared to sampling a single letter? Why?
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{3}
Verify your answer from (a) by writing a program that computes the entropy explicitly. Use \href{https://raw.githubusercontent.com/cschaffner/InformationTheory/master/2019/Text/Jack.txt}{the same text file} as input.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{1}
In the \href{https://canvas.uva.nl/courses/10933/assignments/72715}{second programming quiz} for this module, you computed the entropy of the second letter given the first when sampling a pair of letters from the story of Jack and the beanstalk. Let $P_{XY}$ denote the joint distribution where $X$ is the first and $Y$ is the second letter (again negleticing the beginning and end of the text).

Explain in words why $P_X = P_Y$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{2}
For $P_{XY}$ the bigram distribution from above, write a program to compute the cross entropy $H_C(P_{XY} , P_X \cdot P_Y)$ between the bigram distribution and the independent single-letter distribution.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{1}
Express in words what this quantity  $H_C(P_{XY} , P_X \cdot P_Y)$ means.
\end{subproblem}

\begin{solution}
...
\end{solution}

\end{document}
